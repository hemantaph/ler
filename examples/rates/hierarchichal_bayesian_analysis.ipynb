{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b151e0b",
   "metadata": {},
   "source": [
    "Below is a clean “from-first-principles” explanation of what the handwritten note is doing, with the key goal:\n",
    "\n",
    "> infer the hyper-parameter $\\tau$ that controls the source redshift distribution $p(z_s\\mid\\tau)$, using multiple GW datasets $\\{\\vec d_i\\}_{i=1}^{N}$, including the selection effect (i.e. conditioning on detection).\n",
    "\n",
    "The notation follows the note, where the subscript “0” means Bilby PE done with a simplified prior $p_0(\\theta)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Event-level inference (Bilby PE): what $p_0(\\theta\\mid d)$ means\n",
    "\n",
    "For one event with data $\\vec d$, Bilby returns posterior samples from\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p_0(\\theta \\mid \\vec d)\n",
    "=\\frac{p_0(\\vec d\\mid \\theta)\\,p_0(\\theta)}{p_0(\\vec d)}\\,,\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\theta$ = event parameters (masses, spins, distance or $z$, sky location, etc.)\n",
    "- $p_0(\\theta)$ = the Bilby sampling prior (simplified)\n",
    "- $p_0(\\vec d)$ = the evidence under that prior\n",
    "\n",
    "The evidence is\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p_0(\\vec d)=\\int p_0(\\vec d\\mid\\theta)\\,p_0(\\theta)\\,d\\theta.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "This “0” posterior is not yet a population inference result; it is just the per-event posterior under the PE prior.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Population model: what you want to infer\n",
    "\n",
    "You want a population model in which the source redshift distribution is described by a hyper-parameter $\\tau$:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(z_s\\mid\\tau).\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "In practice, the population model is over all event parameters:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\theta\\mid\\tau),\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "and $\\theta$ contains $z_s$ (and everything else).\n",
    "\n",
    "### Common factorization\n",
    "Often one uses\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\theta\\mid\\tau)\n",
    "=\n",
    "p(z_s\\mid\\tau)\\,p(\\lambda)\\,,\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "where $\\lambda$ denotes the “other parameters” (masses/spins/orientation), possibly with their own hyper-parameters (but you can keep them fixed if the only target is $z_s$).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Where selection effects enter (the conceptual point)\n",
    "\n",
    "The catalog you analyze is a set of detected events, not “all events that occurred”.\n",
    "\n",
    "So the correct likelihood is always conditioned on detection:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\vec d \\mid \\tau, {\\rm det}).\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "This introduces a normalization by the detection efficiency of the population, written as $\\alpha(\\tau)$ (some notes call it $\\sigma(\\tau)$ or $\\beta(\\tau)$).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Single-event likelihood with selection (the role of $\\sigma$)\n",
    "\n",
    "A convenient way to write the single-event contribution is\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\vec d_i \\mid \\tau, {\\rm det})\n",
    "=\n",
    "\\frac{\n",
    "\\int p(\\vec d_i \\mid \\theta)\\,p(\\theta\\mid\\tau)\\,d\\theta\n",
    "}{\n",
    "\\alpha(\\tau)\n",
    "}\\,,\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "where the selection function is\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha(\\tau)\n",
    "=\n",
    "p({\\rm det}\\mid\\tau)\n",
    "=\n",
    "\\int p({\\rm det}\\mid\\theta)\\,p(\\theta\\mid\\tau)\\,d\\theta.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Numerator: “support of the data under the population prior”\n",
    "- Denominator: “fraction of the population that would be detected”\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Multiple-event hierarchical posterior for $\\tau$\n",
    "\n",
    "For $N$ independent detected events $\\{\\vec d_i\\}$, the hierarchical posterior is\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\tau \\mid \\{\\vec d_i\\}, {\\rm det})\n",
    "\\propto\n",
    "p(\\tau)\\,\n",
    "\\prod_{i=1}^{N}\n",
    "p(\\vec d_i \\mid \\tau, {\\rm det}).\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "Substituting the selection-corrected event likelihood gives the catalog likelihood\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\tau)\n",
    "=\n",
    "\\prod_{i=1}^N\n",
    "\\frac{\n",
    "\\int p(\\vec d_i \\mid \\theta)\\,p(\\theta\\mid\\tau)\\,d\\theta\n",
    "}{\n",
    "\\alpha(\\tau)\n",
    "}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "---\n",
    "\n",
    "## 6) The key computational trick: reuse Bilby PE samples\n",
    "\n",
    "Bilby gives\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p_0(\\theta \\mid \\vec d_i)\n",
    "=\n",
    "\\frac{p(\\vec d_i\\mid \\theta)\\,p_0(\\theta)}{p_0(\\vec d_i)}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "Rearranging,\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\vec d_i\\mid \\theta)\n",
    "=\n",
    "\\frac{p_0(\\theta \\mid \\vec d_i)\\,p_0(\\vec d_i)}{p_0(\\theta)}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "Plug into the numerator integral:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "\\int p(\\vec d_i\\mid \\theta)\\,p(\\theta\\mid\\tau)\\,d\\theta\n",
    "=\n",
    "p_0(\\vec d_i)\\,\n",
    "\\int p_0(\\theta\\mid\\vec d_i)\\,\n",
    "\\frac{p(\\theta\\mid\\tau)}{p_0(\\theta)}\\,d\\theta.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "Approximate the integral using posterior samples $\\{\\theta_{ik}\\}_{k=1}^{n_i}\\sim p_0(\\theta\\mid\\vec d_i)$:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "\\int p_0(\\theta\\mid\\vec d_i)\\,\n",
    "\\frac{p(\\theta\\mid\\tau)}{p_0(\\theta)}\\,d\\theta\n",
    "\\approx\n",
    "\\frac{1}{n_i}\\sum_{k=1}^{n_i}\n",
    "\\frac{p(\\theta_{ik}\\mid\\tau)}{p_0(\\theta_{ik})}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "Therefore, up to $\\tau$-independent constants,\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\vec d_i \\mid \\tau, {\\rm det})\n",
    "\\propto\n",
    "\\frac{1}{\\alpha(\\tau)}\n",
    "\\left\\langle\n",
    "\\frac{p(\\theta\\mid\\tau)}{p_0(\\theta)}\n",
    "\\right\\rangle_{\\theta\\sim p_0(\\theta\\mid \\vec d_i)}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "The Bilby evidences $p_0(\\vec d_i)$ drop out because they do not depend on $\\tau$.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Special case: redshift-only hyper-inference\n",
    "\n",
    "If only the redshift part changes and everything else matches the PE prior, then\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{p(\\theta\\mid\\tau)}{p_0(\\theta)}\n",
    "\\approx\n",
    "\\frac{p(z_s\\mid\\tau)}{p_0(z_s)}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "So the event contribution becomes\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\vec d_i \\mid \\tau, {\\rm det})\n",
    "\\propto\n",
    "\\frac{1}{\\alpha(\\tau)}\n",
    "\\left\\langle\n",
    "\\frac{p(z_s\\mid\\tau)}{p_0(z_s)}\n",
    "\\right\\rangle_{z_s\\sim p_0(z_s\\mid \\vec d_i)}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "and the catalog likelihood is\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathcal{L}(\\tau)\n",
    "\\propto\n",
    "\\frac{1}{\\alpha(\\tau)^N}\n",
    "\\prod_{i=1}^N\n",
    "\\left\\langle\n",
    "\\frac{p(z_s\\mid\\tau)}{p_0(z_s)}\n",
    "\\right\\rangle_{z_s\\sim p_0(z_s\\mid \\vec d_i)}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "---\n",
    "\n",
    "## 8) How to compute $\\alpha(\\tau)$ in practice (injections)\n",
    "\n",
    "The selection function is\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha(\\tau)=\\int p({\\rm det}\\mid\\theta)\\,p(\\theta\\mid\\tau)\\,d\\theta.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "Monte Carlo with injections $\\theta_j\\sim p_{\\rm inj}(\\theta)$ gives\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "\\alpha(\\tau)\\approx\n",
    "\\frac{1}{N_{\\rm inj}}\n",
    "\\sum_{j=1}^{N_{\\rm inj}}\n",
    "p({\\rm det}\\mid\\theta_j)\\,\n",
    "\\frac{p(\\theta_j\\mid\\tau)}{p_{\\rm inj}(\\theta_j)}.\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Final one-line formula (what you implement)\n",
    "\n",
    "The hierarchical posterior is\n",
    "\n",
    "$$ \\begin{equation}\n",
    "\\begin{split}\n",
    "p(\\tau \\mid \\{\\vec d_i\\}, {\\rm det})\n",
    "\\propto\n",
    "p(\\tau)\\,\n",
    "\\frac{1}{\\alpha(\\tau)^N}\n",
    "\\prod_{i=1}^N\n",
    "\\left[\n",
    "\\frac{1}{n_i}\\sum_{k=1}^{n_i}\n",
    "\\frac{p(\\theta_{ik}\\mid\\tau)}{p_0(\\theta_{ik})}\n",
    "\\right].\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "This matches the structure in the handwritten note: reweight each event’s PE posterior samples by the ratio of the population prior to the PE prior, then divide by the selection function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc92e2a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
